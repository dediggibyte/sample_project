# databricks.yml
bundle:
  name: sample-job-bundle

targets:
  dev:
    mode: production
    # Don’t interpolate here—Databricks CLI v2 will pick up DATABRICKS_HOST/TOKEN
    # from the environment automatically.
    workspace: {}

artifacts:
  my_library_wheel:
    type: whl
    path: .

resources:
  jobs:
    my_notebook_job:
      name: "Sample Notebook Job (Bundle Deployed)"

      # Define your clusters at the job level...
      job_clusters:
        default_cluster:
          new_cluster:
            spark_version: "16.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1

      # ...and reference them in each task by key
      tasks:
        - task_key: run_notebook_using_library
          notebook_task:
            notebook_path: ./notebooks/job_notebook.py
            source: WORKSPACE
          libraries:
            - whl: ./dist/mylibrary-*.whl
          job_cluster_key: default_cluster
